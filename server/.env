# ============================================================================
# ENVIRONMENT VARIABLES CONFIGURATION
# ============================================================================
# Copy this file to .env and customize if needed
# This configuration uses Ollama for local LLM inference

# Ollama Configuration
# The model name should match one you've installed with: ollama pull <model-name>
# Available models you have: functiongemma
# Recommended models: llama3.2 (8B), mistral (7B), phi3 (3.8B), codellama (7B)
# To install: ollama pull llama3.2
OLLAMA_MODEL=functiongemma
OLLAMA_BASE_URL=http://localhost:11434/v1

# Model Parameters
TEMPERATURE=0.7
TIMEOUT=300

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
